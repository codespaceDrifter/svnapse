0:continual learning within a dataset: in a big dataset like wikipedia with lots of unrelated subjects maybe i can divide the dataset itself into bins and label them like wikipedia[0] - [n] and continual learning on them. cause just like mixing dataset is bad cause the model can't learn a bit of everything at once randomly directioness, having a single dataset with so many different knowledge without dividing it and learning parts of it first is bad. even among homogenius problems like addition perhaps it makes sense to break that problem down into difficulty bins (amount of digits)  

this could be a paper really. find the optimal spit number to learn wikipedia and arithmatic given a number of training batch number for each type of splits how low can loss go.  

1: see what EWC is for continual learning  

1: actual eval.py with the SAME bin file!!! 
